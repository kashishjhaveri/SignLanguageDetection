{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'SignLanguageDetection/Tensorflow/workspace'\n",
    "SCRIPTS_PATH = 'SignLanguageDetection/Tensorflow/scripts'\n",
    "APIMODEL_PATH = 'models'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
    "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "#     {'name':'hello', 'id':1},\n",
    "#     {'name':'yes', 'id':2},\n",
    "#     {'name':'no', 'id':3},\n",
    "#     {'name':'sorry', 'id':4},\n",
    "#     {'name':'thank you', 'id':5},\n",
    "    {'name':'0', 'id':6},\n",
    "    {'name':'1', 'id':7},\n",
    "    {'name':'2', 'id':8},\n",
    "    {'name':'3', 'id':9},\n",
    "    {'name':'4', 'id':10},\n",
    "    {'name':'5', 'id':11},\n",
    "    {'name':'6', 'id':12},\n",
    "    {'name':'7', 'id':13},\n",
    "    {'name':'8', 'id':14},\n",
    "    {'name':'9', 'id':15},\n",
    "    {'name':'a', 'id':16},\n",
    "    {'name':'b', 'id':17},\n",
    "    {'name':'c', 'id':18},\n",
    "    {'name':'d', 'id':19},\n",
    "    {'name':'e', 'id':20},\n",
    "    {'name':'f', 'id':21},\n",
    "    {'name':'g', 'id':22},\n",
    "    {'name':'h', 'id':23},\n",
    "    {'name':'i', 'id':24},\n",
    "    {'name':'j', 'id':25},\n",
    "    {'name':'k', 'id':26},\n",
    "    {'name':'l', 'id':27},\n",
    "    {'name':'m', 'id':28},\n",
    "    {'name':'n', 'id':29},\n",
    "    {'name':'o', 'id':30},\n",
    "    {'name':'p', 'id':31},\n",
    "    {'name':'q', 'id':32},\n",
    "    {'name':'r', 'id':33},\n",
    "    {'name':'s', 'id':34},\n",
    "    {'name':'t', 'id':35},\n",
    "    {'name':'u', 'id':36},\n",
    "    {'name':'v', 'id':37},\n",
    "    {'name':'w', 'id':38},\n",
    "    {'name':'x', 'id':39},\n",
    "    {'name':'y', 'id':40},\n",
    "    {'name':'z', 'id':41}\n",
    "]\n",
    "\n",
    "with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create TF records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: SignLanguageDetection/Tensorflow/workspace/annotations/train.record\n",
      "Successfully created the TFRecord file: SignLanguageDetection/Tensorflow/workspace/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Copy Model Config to Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file SignLanguageDetection\\Tensorflow\\workspace\\models\\my_ssd_mobnet already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "!mkdir {'SignLanguageDetection\\Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}\n",
    "!copy {PRETRAINED_MODEL_PATH.replace('/',\"\\\\\") +'\\\\ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8\\\\pipeline.config'} {MODEL_PATH.replace('/',\"\\\\\")+'\\\\'+CUSTOM_MODEL_NAME.replace('/',\"\\\\\")}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Update Config For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 36\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 320\n",
       "       width: 320\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_mobilenet_v2_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 3.9999998989515007e-05\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         random_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.009999999776482582\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.996999979019165\n",
       "         scale: true\n",
       "         epsilon: 0.0010000000474974513\n",
       "       }\n",
       "     }\n",
       "     use_depthwise: true\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       additional_layer_depth: 128\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 3.9999998989515007e-05\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.009999999776482582\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.996999979019165\n",
       "           scale: true\n",
       "           epsilon: 0.0010000000474974513\n",
       "         }\n",
       "       }\n",
       "       depth: 128\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.599999904632568\n",
       "       share_prediction_tower: true\n",
       "       use_depthwise: true\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 9.99999993922529e-09\n",
       "       iou_threshold: 0.6000000238418579\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "       use_static_shapes: false\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " },\n",
       " 'train_config': batch_size: 25\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.07999999821186066\n",
       "         total_steps: 50000\n",
       "         warmup_learning_rate: 0.026666000485420227\n",
       "         warmup_steps: 1000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.8999999761581421\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
       " num_steps: 50000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"detection\"\n",
       " fine_tune_checkpoint_version: V2,\n",
       " 'train_input_config': label_map_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/train.record\"\n",
       " },\n",
       " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false,\n",
       " 'eval_input_configs': [label_map_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/test.record\"\n",
       " }\n",
       " ],\n",
       " 'eval_input_config': label_map_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/test.record\"\n",
       " }}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = 36\n",
    "pipeline_config.train_config.batch_size = 25\n",
    "pipeline_config.train_config.fine_tune_checkpoint = \"C://Users/kashi/\"+PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= \"C://Users/kashi/\" + ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [\"C://Users/kashi/\"+ANNOTATION_PATH + '/train.record']\n",
    "pipeline_config.eval_input_reader[0].label_map_path =\"C://Users/kashi/\"+ ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [\"C://Users/kashi/\"+ANNOTATION_PATH + '/test.record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python models/research/object_detection/model_main_tf2.py --model_dir=SignLanguageDetection/Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=SignLanguageDetection/Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=10000\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=10000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Load Train Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-6')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 36\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 320\n",
       "       width: 320\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_mobilenet_v2_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 3.9999998989515007e-05\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         random_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.009999999776482582\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.996999979019165\n",
       "         scale: true\n",
       "         epsilon: 0.0010000000474974513\n",
       "       }\n",
       "     }\n",
       "     use_depthwise: true\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       additional_layer_depth: 128\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 3.9999998989515007e-05\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.009999999776482582\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.996999979019165\n",
       "           scale: true\n",
       "           epsilon: 0.0010000000474974513\n",
       "         }\n",
       "       }\n",
       "       depth: 128\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.599999904632568\n",
       "       share_prediction_tower: true\n",
       "       use_depthwise: true\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 9.99999993922529e-09\n",
       "       iou_threshold: 0.6000000238418579\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "       use_static_shapes: false\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " },\n",
       " 'train_config': batch_size: 25\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.07999999821186066\n",
       "         total_steps: 50000\n",
       "         warmup_learning_rate: 0.026666000485420227\n",
       "         warmup_steps: 1000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.8999999761581421\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
       " num_steps: 50000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"detection\"\n",
       " fine_tune_checkpoint_version: V2,\n",
       " 'train_input_config': label_map_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/train.record\"\n",
       " },\n",
       " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false,\n",
       " 'eval_input_configs': [label_map_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/test.record\"\n",
       " }\n",
       " ],\n",
       " 'eval_input_config': label_map_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"C://Users/kashi/SignLanguageDetection/Tensorflow/workspace/annotations/test.record\"\n",
       " }}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Detect in Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import numpy as np\n",
    "from object_detection.utils import label_map_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: {'id': 6, 'name': '0'},\n",
       " 7: {'id': 7, 'name': '1'},\n",
       " 8: {'id': 8, 'name': '2'},\n",
       " 9: {'id': 9, 'name': '3'},\n",
       " 10: {'id': 10, 'name': '4'},\n",
       " 11: {'id': 11, 'name': '5'},\n",
       " 12: {'id': 12, 'name': '6'},\n",
       " 13: {'id': 13, 'name': '7'},\n",
       " 14: {'id': 14, 'name': '8'},\n",
       " 15: {'id': 15, 'name': '9'},\n",
       " 16: {'id': 16, 'name': 'a'},\n",
       " 17: {'id': 17, 'name': 'b'},\n",
       " 18: {'id': 18, 'name': 'c'},\n",
       " 19: {'id': 19, 'name': 'd'},\n",
       " 20: {'id': 20, 'name': 'e'},\n",
       " 21: {'id': 21, 'name': 'f'},\n",
       " 22: {'id': 22, 'name': 'g'},\n",
       " 23: {'id': 23, 'name': 'h'},\n",
       " 24: {'id': 24, 'name': 'i'},\n",
       " 25: {'id': 25, 'name': 'j'},\n",
       " 26: {'id': 26, 'name': 'k'},\n",
       " 27: {'id': 27, 'name': 'l'},\n",
       " 28: {'id': 28, 'name': 'm'},\n",
       " 29: {'id': 29, 'name': 'n'},\n",
       " 30: {'id': 30, 'name': 'o'},\n",
       " 31: {'id': 31, 'name': 'p'},\n",
       " 32: {'id': 32, 'name': 'q'},\n",
       " 33: {'id': 33, 'name': 'r'},\n",
       " 34: {'id': 34, 'name': 's'},\n",
       " 35: {'id': 35, 'name': 't'},\n",
       " 36: {'id': 36, 'name': 'u'},\n",
       " 37: {'id': 37, 'name': 'v'},\n",
       " 38: {'id': 38, 'name': 'w'},\n",
       " 39: {'id': 39, 'name': 'x'},\n",
       " 40: {'id': 40, 'name': 'y'},\n",
       " 41: {'id': 41, 'name': 'z'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while True: \n",
    "    ret, frame = cap.read()\n",
    "    image_np = np.array(frame)\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    #print(input_tensor)\n",
    "    detections = detect_fn(input_tensor)\n",
    "\n",
    "    num_detections = int(detections.pop('num_detections'))\n",
    "    detections = {key: value[0, :num_detections].numpy()\n",
    "                  for key, value in detections.items()}\n",
    "    detections['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "    label_id_offset = 1\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=5,\n",
    "                min_score_thresh=.5,\n",
    "                agnostic_mode=False)\n",
    "\n",
    "    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 1000)))\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detect_fn(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_boxes': <tf.Tensor: shape=(1, 100, 4), dtype=float32, numpy=\n",
       " array([[[6.02740049e-03, 0.00000000e+00, 1.00000000e+00, 9.86452281e-01],\n",
       "         [1.54173374e-03, 1.35249496e-02, 9.89860296e-01, 9.90350187e-01],\n",
       "         [1.56825781e-03, 1.75031126e-02, 9.91923273e-01, 9.84224319e-01],\n",
       "         [0.00000000e+00, 2.97385454e-03, 9.91284311e-01, 9.90302861e-01],\n",
       "         [5.37276268e-04, 0.00000000e+00, 1.00000000e+00, 9.97304797e-01],\n",
       "         [6.02740049e-03, 0.00000000e+00, 1.00000000e+00, 9.86452281e-01],\n",
       "         [6.02740049e-03, 0.00000000e+00, 1.00000000e+00, 9.86452281e-01],\n",
       "         [8.05066228e-02, 1.59652829e-01, 7.16217339e-01, 9.93894339e-01],\n",
       "         [1.56825781e-03, 1.75031126e-02, 9.91923273e-01, 9.84224319e-01],\n",
       "         [1.56825781e-03, 1.75031126e-02, 9.91923273e-01, 9.84224319e-01],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [0.00000000e+00, 1.65634453e-02, 6.80566907e-01, 1.00000000e+00],\n",
       "         [1.50298476e-02, 3.17485303e-01, 8.55067909e-01, 9.76412773e-01],\n",
       "         [0.00000000e+00, 1.65634453e-02, 6.80566907e-01, 1.00000000e+00],\n",
       "         [6.89610839e-03, 5.90831041e-04, 9.99012470e-01, 9.81904864e-01],\n",
       "         [4.93052602e-02, 5.69761097e-02, 8.12375069e-01, 7.18024254e-01],\n",
       "         [1.84874207e-01, 0.00000000e+00, 8.75197887e-01, 5.89671254e-01],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [6.89610839e-03, 5.90831041e-04, 9.99012470e-01, 9.81904864e-01],\n",
       "         [0.00000000e+00, 1.65634453e-02, 6.80566907e-01, 1.00000000e+00],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [1.64062381e-02, 4.06884760e-01, 8.98511887e-01, 9.99321699e-01],\n",
       "         [0.00000000e+00, 1.65634453e-02, 6.80566907e-01, 1.00000000e+00],\n",
       "         [1.87283918e-01, 3.40891391e-01, 6.85655653e-01, 9.21963811e-01],\n",
       "         [3.69077682e-01, 0.00000000e+00, 8.79102707e-01, 6.30233645e-01],\n",
       "         [1.99254662e-01, 0.00000000e+00, 7.33442187e-01, 6.50168002e-01],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [1.99848115e-02, 3.14104527e-01, 8.78222704e-01, 9.88860965e-01],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [1.60999417e-01, 7.97335804e-02, 5.83477736e-01, 6.29606485e-01],\n",
       "         [3.80198658e-02, 8.98101628e-02, 9.65990901e-01, 1.00000000e+00],\n",
       "         [5.45609295e-02, 0.00000000e+00, 1.00000000e+00, 6.10714912e-01],\n",
       "         [0.00000000e+00, 1.65634453e-02, 6.80566907e-01, 1.00000000e+00],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [5.45609295e-02, 0.00000000e+00, 1.00000000e+00, 6.10714912e-01],\n",
       "         [1.50298476e-02, 3.17485303e-01, 8.55067909e-01, 9.76412773e-01],\n",
       "         [3.90431315e-01, 5.35561740e-02, 1.00000000e+00, 6.68681860e-01],\n",
       "         [7.15498030e-02, 4.16394830e-01, 9.53500271e-01, 1.00000000e+00],\n",
       "         [4.01066422e-01, 0.00000000e+00, 1.00000000e+00, 5.88443160e-01],\n",
       "         [1.13783479e-02, 3.90863121e-02, 9.90114391e-01, 9.55859661e-01],\n",
       "         [5.45609295e-02, 0.00000000e+00, 1.00000000e+00, 6.10714912e-01],\n",
       "         [6.53046370e-01, 1.40245259e-02, 1.00000000e+00, 5.46547294e-01],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [1.52418703e-01, 0.00000000e+00, 1.00000000e+00, 6.40659332e-01],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [2.03707874e-01, 4.58416432e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [5.45609295e-02, 0.00000000e+00, 1.00000000e+00, 6.10714912e-01],\n",
       "         [4.13929522e-02, 0.00000000e+00, 1.00000000e+00, 6.90037668e-01],\n",
       "         [0.00000000e+00, 1.65634453e-02, 6.80566907e-01, 1.00000000e+00],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [4.78539735e-01, 2.27485597e-02, 9.62877154e-01, 5.13577580e-01],\n",
       "         [7.71958977e-02, 3.79949808e-03, 5.20233154e-01, 4.94341582e-01],\n",
       "         [5.45609295e-02, 0.00000000e+00, 1.00000000e+00, 6.10714912e-01],\n",
       "         [5.37866950e-02, 0.00000000e+00, 9.52498853e-01, 8.61536384e-01],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [7.80560076e-02, 0.00000000e+00, 9.88545418e-01, 6.19455099e-01],\n",
       "         [6.47157431e-02, 0.00000000e+00, 7.90540457e-01, 5.81298470e-01],\n",
       "         [6.74575716e-02, 1.90969169e-01, 5.53440571e-01, 7.00145960e-01],\n",
       "         [1.77655220e-02, 3.04757953e-02, 1.00000000e+00, 1.00000000e+00],\n",
       "         [4.51998144e-01, 1.61034048e-01, 9.97198701e-01, 5.40438414e-01],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [4.44547713e-01, 0.00000000e+00, 9.66862857e-01, 6.76107168e-01],\n",
       "         [0.00000000e+00, 9.72922146e-03, 2.39731714e-01, 5.04606485e-01],\n",
       "         [1.67213976e-01, 2.94817090e-02, 8.94183457e-01, 6.93769336e-01],\n",
       "         [5.08926511e-02, 4.18942839e-01, 8.09581459e-01, 9.89519715e-01],\n",
       "         [3.21546555e-01, 3.47144902e-02, 7.10204124e-01, 6.24712586e-01],\n",
       "         [6.02740049e-03, 0.00000000e+00, 1.00000000e+00, 9.86452281e-01],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [0.00000000e+00, 1.13377959e-01, 7.01010704e-01, 9.35354590e-01],\n",
       "         [0.00000000e+00, 5.39625049e-01, 6.44304812e-01, 1.00000000e+00],\n",
       "         [3.25428337e-01, 1.60000175e-01, 8.74898314e-01, 9.15344834e-01],\n",
       "         [2.59718120e-01, 1.74401999e-02, 9.57265198e-01, 6.89563632e-01],\n",
       "         [3.53331000e-01, 3.53272766e-01, 8.61089110e-01, 9.07515883e-01],\n",
       "         [0.00000000e+00, 7.85396516e-01, 4.10637975e-01, 1.00000000e+00],\n",
       "         [6.63093626e-02, 4.38470423e-01, 7.16037750e-01, 9.67430055e-01],\n",
       "         [2.69769073e-01, 1.62835926e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [1.15020186e-01, 3.66652817e-01, 6.32983446e-01, 9.03700709e-01],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [5.08926511e-02, 4.18942839e-01, 8.09581459e-01, 9.89519715e-01],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [2.09877044e-01, 0.00000000e+00, 1.00000000e+00, 7.82472491e-01],\n",
       "         [6.79532290e-02, 1.84756577e-01, 6.62944198e-01, 9.70987499e-01],\n",
       "         [0.00000000e+00, 3.59453022e-01, 5.66260934e-01, 9.03028667e-01],\n",
       "         [0.00000000e+00, 1.65634453e-02, 6.80566907e-01, 1.00000000e+00],\n",
       "         [2.69769073e-01, 1.62835926e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [1.28534734e-02, 3.62511635e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [5.08926511e-02, 4.18942839e-01, 8.09581459e-01, 9.89519715e-01],\n",
       "         [5.29052854e-01, 8.32021236e-03, 1.00000000e+00, 5.78867018e-01],\n",
       "         [5.35978377e-01, 8.72805417e-02, 1.00000000e+00, 6.65359020e-01],\n",
       "         [2.99638510e-03, 2.30848789e-02, 9.78768766e-01, 9.92757916e-01],\n",
       "         [1.31174922e-01, 2.65848845e-01, 5.99479675e-01, 8.37162614e-01],\n",
       "         [3.53062689e-01, 1.33227304e-01, 8.82799208e-01, 5.34718394e-01],\n",
       "         [1.09008819e-01, 4.09591675e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [0.00000000e+00, 4.38966155e-01, 8.79545569e-01, 1.00000000e+00],\n",
       "         [4.13929522e-02, 0.00000000e+00, 1.00000000e+00, 6.90037668e-01],\n",
       "         [1.09008819e-01, 4.09591675e-01, 1.00000000e+00, 1.00000000e+00],\n",
       "         [1.52268410e-01, 3.50874633e-01, 6.13368630e-01, 8.93271685e-01],\n",
       "         [3.38504642e-01, 4.17966247e-02, 1.00000000e+00, 9.53431606e-01],\n",
       "         [0.00000000e+00, 0.00000000e+00, 6.76105440e-01, 6.13559484e-01]]],\n",
       "       dtype=float32)>,\n",
       " 'detection_scores': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       " array([[0.36457506, 0.26456085, 0.24582644, 0.22341886, 0.2126538 ,\n",
       "         0.06034352, 0.05134578, 0.04961297, 0.04368704, 0.03993939,\n",
       "         0.03668446, 0.03639302, 0.03468338, 0.03387044, 0.03285691,\n",
       "         0.0324121 , 0.02933952, 0.02888286, 0.02837743, 0.02782288,\n",
       "         0.02777981, 0.02777929, 0.02770207, 0.02769061, 0.02739817,\n",
       "         0.02734741, 0.02685409, 0.02680243, 0.02665568, 0.02569227,\n",
       "         0.0236929 , 0.02312509, 0.02300585, 0.02296773, 0.02283344,\n",
       "         0.02211648, 0.02174583, 0.02160598, 0.02147064, 0.02098845,\n",
       "         0.02069614, 0.02067618, 0.02030788, 0.02027554, 0.02012037,\n",
       "         0.01965093, 0.01963849, 0.01912201, 0.01904292, 0.01880436,\n",
       "         0.01870661, 0.01841826, 0.01822401, 0.01816743, 0.01813107,\n",
       "         0.01808643, 0.0178366 , 0.01777629, 0.01740764, 0.01730153,\n",
       "         0.01718628, 0.01694035, 0.01689056, 0.01677017, 0.0167186 ,\n",
       "         0.01669517, 0.01657598, 0.01653559, 0.01639946, 0.01627846,\n",
       "         0.01618997, 0.01611113, 0.01609357, 0.0160085 , 0.01599683,\n",
       "         0.01597138, 0.0158618 , 0.01567306, 0.01553352, 0.01551255,\n",
       "         0.01530257, 0.01523541, 0.01521196, 0.01517829, 0.01478241,\n",
       "         0.0147479 , 0.01473707, 0.01473569, 0.01454746, 0.01448468,\n",
       "         0.0144676 , 0.01439596, 0.01430784, 0.01429023, 0.01416782,\n",
       "         0.01394978, 0.01394504, 0.01394145, 0.01386096, 0.01378321]],\n",
       "       dtype=float32)>,\n",
       " 'detection_classes': <tf.Tensor: shape=(1, 100), dtype=float32, numpy=\n",
       " array([[31., 28., 27., 30., 29., 17., 33., 30., 34., 15., 27., 28., 35.,\n",
       "         34., 24., 30., 24., 31., 22., 29., 30., 32., 30., 31., 30., 30.,\n",
       "         29., 31., 28., 30., 23., 31., 31., 27., 30., 24., 30., 20., 24.,\n",
       "          6., 34., 24., 15., 29., 33.,  6., 28., 32., 27., 22., 30., 30.,\n",
       "         27.,  5., 28., 25., 17., 30., 32., 31., 19., 31., 30., 31., 28.,\n",
       "         30., 14., 33., 17., 20., 30., 22., 31., 25., 33., 29., 24., 29.,\n",
       "         27., 34., 25., 27., 30., 33., 28., 34., 17.,  5., 31., 22., 20.,\n",
       "         30., 30., 16.,  6., 11.,  6., 33., 15., 34.]], dtype=float32)>,\n",
       " 'num_detections': <tf.Tensor: shape=(1,), dtype=float32, numpy=array([100.], dtype=float32)>,\n",
       " 'raw_detection_boxes': <tf.Tensor: shape=(1, 12804, 4), dtype=float32, numpy=\n",
       " array([[[-0.03904378, -0.03485186,  0.07096472,  0.06164125],\n",
       "         [-0.04729579, -0.04313578,  0.08108465,  0.07233227],\n",
       "         [-0.02687741, -0.04795308,  0.05854492,  0.07650761],\n",
       "         ...,\n",
       "         [-0.1521439 , -0.739212  ,  1.6687739 ,  2.0955348 ],\n",
       "         [-0.30210423,  0.09457654,  1.7664328 ,  1.3476126 ],\n",
       "         [-0.65953815, -0.11514091,  1.8909954 ,  1.6073878 ]]],\n",
       "       dtype=float32)>,\n",
       " 'raw_detection_scores': <tf.Tensor: shape=(1, 12804, 37), dtype=float32, numpy=\n",
       " array([[[0.00345378, 0.00228029, 0.00253573, ..., 0.00132062,\n",
       "          0.00117641, 0.00187223],\n",
       "         [0.00279596, 0.00221029, 0.00313272, ..., 0.00102091,\n",
       "          0.0012089 , 0.0028146 ],\n",
       "         [0.00312279, 0.00257099, 0.00307276, ..., 0.00183677,\n",
       "          0.00149639, 0.00248514],\n",
       "         ...,\n",
       "         [0.00490046, 0.0047071 , 0.00422084, ..., 0.00379413,\n",
       "          0.00274234, 0.00509093],\n",
       "         [0.00482351, 0.00285525, 0.00442896, ..., 0.00684131,\n",
       "          0.00375209, 0.00430361],\n",
       "         [0.0037643 , 0.00490812, 0.00475231, ..., 0.00465876,\n",
       "          0.00244918, 0.00352876]]], dtype=float32)>,\n",
       " 'detection_multiclass_scores': <tf.Tensor: shape=(1, 100, 37), dtype=float32, numpy=\n",
       " array([[[0.00167214, 0.00161317, 0.00114445, ..., 0.05134578,\n",
       "          0.0328817 , 0.00053695],\n",
       "         [0.00218096, 0.00098425, 0.00041433, ..., 0.01699179,\n",
       "          0.02558003, 0.0017454 ],\n",
       "         [0.00121547, 0.00153208, 0.00066842, ..., 0.04655405,\n",
       "          0.04368704, 0.00034101],\n",
       "         ...,\n",
       "         [0.00139644, 0.0031027 , 0.00171058, ..., 0.01394145,\n",
       "          0.00105851, 0.00060599],\n",
       "         [0.00410939, 0.00356834, 0.00271091, ..., 0.02012037,\n",
       "          0.01551255, 0.00138471],\n",
       "         [0.00543968, 0.00467471, 0.00509285, ..., 0.00670495,\n",
       "          0.01378321, 0.00586226]]], dtype=float32)>,\n",
       " 'detection_anchor_indices': <tf.Tensor: shape=(1, 100), dtype=int32, numpy=\n",
       " array([[12704, 12643, 12644, 12672, 12667, 12704, 12704, 12213, 12644,\n",
       "         12644, 12734, 12614, 12217, 12614, 12707, 12199, 12313, 12734,\n",
       "         12707, 12614, 12734, 12287, 12614, 12276, 12375, 12255, 12734,\n",
       "         12277, 12688, 12200, 12687, 12664, 12614, 12688, 12664, 12217,\n",
       "         12439, 12347, 12433, 12617, 12664, 12553, 12688, 12691, 12734,\n",
       "         12749, 12664, 12661, 12614, 12688, 12432, 12134, 12664, 12663,\n",
       "         12734, 12660, 12193, 12146, 12709, 12442, 12688, 12441, 12014,\n",
       "         12319, 12223, 12320, 12704, 12688, 12612, 12628, 12393, 12379,\n",
       "         12396, 12118, 12163, 12739, 12216, 12688, 12223, 12734, 12693,\n",
       "         12153, 12097, 12614, 12739, 12688, 12734, 12223, 12493, 12499,\n",
       "         12679, 12212, 12382, 12718, 12629, 12661, 12718, 12218, 12734,\n",
       "         12601]])>}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4.0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
